import requests
import pandas as pd
import time

# GitHub API setup
GITHUB_TOKEN = "###its a secreat code and i am unable to upload" 
HEADERS = {"Authorization": f"token {GITHUB_TOKEN}"}
BASE_URL = "https://api.github.com"

# Function to clean company names
def clean_company_name(company):
    if company:
        company = company.strip()
        if company.startswith("@"):
            company = company[1:]
        company = company.upper()
    return company

# Step 1: Fetch users with over 200 followers in Seattle
def fetch_seattle_users(min_followers=200):
    users = []
    url = f"{BASE_URL}/search/users"
    params = {
        "q": f"location:Seattle followers:>{min_followers}",
        "per_page": 30,
        "page": 1,
    }
    
    while True:
        response = requests.get(url, headers=HEADERS, params=params)
        response.raise_for_status()
        data = response.json()
        
        for user_info in data["items"]:
            user_url = user_info["url"]
            user_data = requests.get(user_url, headers=HEADERS).json()
            users.append({
                "login": user_data.get("login"),
                "name": user_data.get("name") or "",
                "company": clean_company_name(user_data.get("company", "")),
                "location": user_data.get("location") or "",
                "email": user_data.get("email") or "",
                "hireable": user_data.get("hireable") if user_data.get("hireable") is not None else "",
                "bio": user_data.get("bio") or "",
                "public_repos": user_data.get("public_repos"),
                "followers": user_data.get("followers"),
                "following": user_data.get("following"),
                "created_at": user_data.get("created_at"),
            })
        
        if "next" not in response.links:
            break
        params["page"] += 1
        time.sleep(1)  # To avoid rate limit
    
    return users

# Step 2: Fetch repositories for each user
def fetch_user_repos(username, max_repos=500):
    repos = []
    url = f"{BASE_URL}/users/{username}/repos"
    params = {
        "sort": "pushed",
        "per_page": 100,
        "page": 1,
    }
    
    while len(repos) < max_repos:
        response = requests.get(url, headers=HEADERS, params=params)
        response.raise_for_status()
        data = response.json()
        
        for repo in data:
            repos.append({
                "login": username,
                "full_name": repo.get("full_name"),
                "created_at": repo.get("created_at"),
                "stargazers_count": repo.get("stargazers_count"),
                "watchers_count": repo.get("watchers_count"),
                "language": repo.get("language") or "",
                "has_projects": repo.get("has_projects"),
                "has_wiki": repo.get("has_wiki"),
                "license_name": repo["license"]["name"] if repo.get("license") else ""
            })
        
        if "next" not in response.links or len(repos) >= max_repos:
            break
        params["page"] += 1
        time.sleep(1)  # To avoid rate limit
    
    return repos[:max_repos]

# Step 3: Save data to CSV files
def save_to_csv(users, repositories):
    users_df = pd.DataFrame(users)
    users_df.to_csv("users.csv", index=False)

    repos_df = pd.DataFrame(repositories)
    repos_df.to_csv("repositories.csv", index=False)

# Main execution
if __name__ == "__main__":
    # Fetch users
    print("Fetching Seattle users with over 200 followers...")
    users = fetch_seattle_users()
    print(f"Fetched {len(users)} users.")

    # Fetch repositories for each user
    print("Fetching repositories for each user...")
    repositories = []
    for user in users:
        username = user["login"]
        user_repos = fetch_user_repos(username)
        repositories.extend(user_repos)
        print(f"Fetched {len(user_repos)} repositories for user {username}.")

    # Save to CSV
    print("Saving data to CSV files...")
    save_to_csv(users, repositories)
    print("Data saved successfully!")
